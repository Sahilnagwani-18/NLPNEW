{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d59fcfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7634740c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")  # sentence and word tokenization data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8e59482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def sentence_tokenizer(paragraph):\n",
    "  pattern='[\\u0964!?.][^0-9.0-9]'\n",
    "  sentences=re.split(pattern,paragraph)\n",
    "  ends=re.findall(pattern,paragraph)\n",
    "  res=[]\n",
    "  for sentence in sentences:\n",
    "    if ends==[]:\n",
    "      break\n",
    "    end=ends.pop(0)\n",
    "    res.append(sentence.strip() + end)\n",
    "  return res\n",
    "\n",
    "\n",
    "def word_tokenizer(sentence):\n",
    "    url_pattern = r'https?://(?:www\\.)?[a-zA-Z0-9-]+\\.[a-zA-Z]{2,}'\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "  \n",
    "    phone_pattern = r'(?:\\+?[0-9\\u0966-\\u096F]{1,3}[\\s-]?)?[0-9\\u0966-\\u096F]{10}'\n",
    "\n",
    "    # Time format: 10:30 or १०:३०\n",
    "    time_pattern = r'[0-9\\u0966-\\u096F]{1,2}:[0-9\\u0966-\\u096F]{2}'\n",
    "\n",
    "    # Decimal and whole numbers\n",
    "    latin_number = r'[0-9]+(?:\\.[0-9]+)?'\n",
    "    devnag_number = r'[\\u0966-\\u096F]+(?:\\.[\\u0966-\\u096F]+)?'\n",
    "\n",
    "    # Punctuation\n",
    "    punctuation = r'[!?\\.\\u0964\\,\"\\'\\+-]'\n",
    "\n",
    "    # Hindi / Devanagari words\n",
    "    hindi_word = fr'[\\u0900-\\u0963\\u0965-\\u096F]+[^!?\\.\\u0964\\,\"\\'\\+-]'\n",
    "    english_word = r'[a-zA-Z]+'\n",
    "\n",
    "\n",
    "    # Emojis (basic unicode emoji range)\n",
    "    emoji_pattern = r'[\\U0001F300-\\U0001FAFF\\U00002700-\\U000027BF]'\n",
    "\n",
    "    # Final combined pattern\n",
    "    pattern = fr'{email_pattern}|{url_pattern}|{phone_pattern}|{time_pattern}|{latin_number}|{devnag_number}|{punctuation}|{hindi_word}|{emoji_pattern}|{english_word}'\n",
    "\n",
    "    return re.findall(pattern, sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ed9ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db0b4e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags saved to: output_pos_tags\\pos_tags.txt\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n",
    "# Read tokens from file\n",
    "with open(\"brown_nouns.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Filter stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# POS tagging\n",
    "tagged = nltk.pos_tag(filtered_tokens)\n",
    "\n",
    "# Save output to folder\n",
    "output_folder = \"output_pos_tags\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_path = os.path.join(output_folder, \"pos_tags.txt\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for word, tag in tagged:\n",
    "        f.write(f\"{word}\\t{tag}\\n\")\n",
    "\n",
    "print(f\"POS tags saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dce0c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341c3cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Results written to brown_nouns_morphs.txt\n",
      "\n",
      "Sample outputs:\n",
      "fox -> fox+N+SG\n",
      "foxes -> fox+N+PL\n",
      "foxs -> Invalid Word\n",
      "try -> try+N+SG\n",
      "bus -> bus+N+SG\n",
      "busss -> Invalid Word\n",
      "boys -> boy+N+PL\n",
      "kindness -> kindness+N+SG\n",
      "Implementation -> implementation+N+SG\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IRREGULARS = {\n",
    "    \"children\": (\"child\", \"PL\"),\n",
    "    \"men\": (\"man\", \"PL\"),\n",
    "    \"women\": (\"woman\", \"PL\"),\n",
    "    \"people\": (\"person\", \"PL\"),\n",
    "    \"mice\": (\"mouse\", \"PL\"),\n",
    "    \"geese\": (\"goose\", \"PL\"),\n",
    "    \"teeth\": (\"tooth\", \"PL\"),\n",
    "    \"feet\": (\"foot\", \"PL\"),\n",
    "    \"oxen\": (\"ox\", \"PL\"),\n",
    "}\n",
    "\n",
    "VOWELS = set(\"aeiou\")\n",
    "WORD_RE = re.compile(r\"^[a-z]+$\") \n",
    "\n",
    "def load_words(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    tokens = [t.strip().lower() for t in re.split(r\"\\s+\", text) if t.strip()]\n",
    "    tokens = [t for t in tokens if WORD_RE.match(t)]\n",
    "    return sorted(set(tokens))\n",
    "\n",
    "\n",
    "def build_candidate_roots(all_words):\n",
    "    all_set = set(all_words)\n",
    "    roots = set()\n",
    "\n",
    "    for w in all_words:\n",
    "        if not w.endswith(\"s\"):\n",
    "            roots.add(w)\n",
    "\n",
    "    for w in all_words:\n",
    "        if w.endswith(\"ies\"):\n",
    "            cand = w[:-3] + \"y\"\n",
    "            if cand in all_set:\n",
    "                roots.add(cand)\n",
    "        if w.endswith(\"es\"):\n",
    "            cand = w[:-2]\n",
    "            if cand in all_set:\n",
    "                roots.add(cand)\n",
    "        if w.endswith(\"s\"):\n",
    "            cand = w[:-1]\n",
    "            if cand in all_set:\n",
    "                roots.add(cand)\n",
    "\n",
    " \n",
    "    for surf, (root, num) in IRREGULARS.items():\n",
    "        roots.add(root)\n",
    "\n",
    "    return roots\n",
    "\n",
    "\n",
    "def analyze_word(w, roots):\n",
    "    w = w.lower().strip()\n",
    "    if not w or not WORD_RE.match(w):\n",
    "        return \"Invalid Word\"\n",
    "\n",
    "    if w in IRREGULARS:\n",
    "        root, num = IRREGULARS[w]\n",
    "        return f\"{root}+N+{num}\"\n",
    "\n",
    "\n",
    "    if w in roots:\n",
    "        return f\"{w}+N+SG\"\n",
    "\n",
    "    \n",
    "    if w.endswith(\"ies\"):\n",
    "        stem = w[:-3]\n",
    "        if not stem:\n",
    "            return \"Invalid Word\"\n",
    "        \n",
    "        root = stem + \"y\"\n",
    "        \n",
    "        if stem and (stem[-1] not in VOWELS):\n",
    "            if root in roots:\n",
    "                return f\"{root}+N+PL\"\n",
    "        return \"Invalid Word\"\n",
    "\n",
    "   \n",
    "    if w.endswith(\"es\"):\n",
    "        root_candidate = w[:-2]\n",
    "        if not root_candidate:\n",
    "            return \"Invalid Word\"\n",
    "        if (root_candidate.endswith((\"ch\", \"sh\"))\n",
    "            or (root_candidate and root_candidate[-1] in (\"s\", \"z\", \"x\"))):\n",
    "            if root_candidate in roots:\n",
    "                return f\"{root_candidate}+N+PL\"\n",
    "        return \"Invalid Word\"\n",
    "\n",
    "\n",
    "    if w.endswith(\"s\"):\n",
    "        root_candidate = w[:-1]\n",
    "        if not root_candidate:\n",
    "            return \"Invalid Word\"\n",
    "\n",
    "        \n",
    "        if root_candidate.endswith((\"ch\", \"sh\")) or (root_candidate and root_candidate[-1] in (\"s\", \"z\", \"x\")):\n",
    "            return \"Invalid Word\"\n",
    "\n",
    "        \n",
    "        if root_candidate.endswith(\"y\"):\n",
    "            if len(root_candidate) >= 2 and root_candidate[-2] in VOWELS:\n",
    "                if root_candidate in roots:\n",
    "                    return f\"{root_candidate}+N+PL\"\n",
    "                return \"Invalid Word\"\n",
    "            else:\n",
    "                return \"Invalid Word\"\n",
    "\n",
    "        \n",
    "        if root_candidate in roots:\n",
    "            return f\"{root_candidate}+N+PL\"\n",
    "\n",
    "        return \"Invalid Word\"\n",
    "\n",
    "    \n",
    "    return \"Invalid Word\"\n",
    "\n",
    "\n",
    "def process_corpus(infile=\"brown_nouns.txt\", outfile=\"brown_nouns_morphs.txt\"):\n",
    "    if not os.path.exists(infile):\n",
    "        print(f\"ERROR: input file '{infile}' not found.\", file=sys.stderr)\n",
    "        return\n",
    "\n",
    "    words = load_words(infile)\n",
    "    roots = build_candidate_roots(words)\n",
    "\n",
    "    results = {}\n",
    "    for w in sorted(words):\n",
    "        results[w] = analyze_word(w, roots)\n",
    "\n",
    "    \n",
    "    with open(outfile, \"w\", encoding=\"utf-8\") as out:\n",
    "        for w in sorted(results.keys()):\n",
    "            out.write(f\"{w} = {results[w]}\\n\")\n",
    "\n",
    "    print(f\"Done. Results written to {outfile}\")\n",
    "    \n",
    "    sample = [\"fox\", \"foxes\", \"foxs\", \"try\", \"bus\", \"busss\", \"boys\", \"kindness\", \"Implementation\"]\n",
    "    print(\"\\nSample outputs:\")\n",
    "    for s in sample:\n",
    "        print(f\"{s} -> {analyze_word(s, roots)}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"FST analyzer for brown_nouns\")\n",
    "    parser.add_argument(\"inpath\", nargs=\"?\", default=\"brown_nouns.txt\",\n",
    "                        help=\"Input file (default: brown_nouns.txt)\")\n",
    "    parser.add_argument(\"outpath\", nargs=\"?\", default=\"brown_nouns_morphs.txt\",\n",
    "                        help=\"Output file (default: brown_nouns_morphs.txt)\")\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    inpath = args.inpath\n",
    "    outpath = args.outpath\n",
    "\n",
    "   \n",
    "    process_corpus(infile=inpath, outfile=outpath)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "601096f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyformlang\n",
      "  Downloading pyformlang-1.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from pyformlang) (3.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from pyformlang) (2.1.3)\n",
      "Collecting pydot (from pyformlang)\n",
      "  Downloading pydot-4.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyparsing>=3.1.0 in c:\\users\\sahil\\anaconda3\\lib\\site-packages (from pydot->pyformlang) (3.2.0)\n",
      "Downloading pyformlang-1.0.11-py3-none-any.whl (128 kB)\n",
      "Downloading pydot-4.0.1-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pydot, pyformlang\n",
      "\n",
      "   -------------------- ------------------- 1/2 [pyformlang]\n",
      "   ---------------------------------------- 2/2 [pyformlang]\n",
      "\n",
      "Successfully installed pydot-4.0.1 pyformlang-1.0.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyformlang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1a45974",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'State' from 'pyformlang.fst' (c:\\Users\\sahil\\anaconda3\\Lib\\site-packages\\pyformlang\\fst\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyformlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfst\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FST\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyformlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfst\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m State\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define states\u001b[39;00m\n\u001b[0;32m      5\u001b[0m q0, q1, q2, q3 \u001b[38;5;241m=\u001b[39m State(\u001b[38;5;241m0\u001b[39m), State(\u001b[38;5;241m1\u001b[39m), State(\u001b[38;5;241m2\u001b[39m), State(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'State' from 'pyformlang.fst' (c:\\Users\\sahil\\anaconda3\\Lib\\site-packages\\pyformlang\\fst\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pyformlang.fst import FST\n",
    "from pyformlang.fst import State\n",
    "\n",
    "# Define states\n",
    "q0, q1, q2, q3 = State(0), State(1), State(2), State(3)\n",
    "\n",
    "fst = FST()\n",
    "\n",
    "# Initial state\n",
    "fst.add_start_state(q0)\n",
    "\n",
    "# Final (accepting) state\n",
    "fst.add_final_state(q3)\n",
    "\n",
    "# Transitions for (a|b)*abb\n",
    "# On reading 'a', stay in q0 or progress towards final\n",
    "fst.add_transition(q0, \"a\", \"a\", q0)\n",
    "fst.add_transition(q0, \"b\", \"b\", q0)\n",
    "\n",
    "fst.add_transition(q0, \"a\", \"a\", q1)\n",
    "fst.add_transition(q1, \"b\", \"b\", q2)\n",
    "fst.add_transition(q2, \"b\", \"b\", q3)\n",
    "\n",
    "# Function to test input\n",
    "def test_word(word):\n",
    "    result = list(fst.translate(word))\n",
    "    if result:\n",
    "        print(f\"✅ Accepted: {word} → Output: {result}\")\n",
    "    else:\n",
    "        print(f\"❌ Rejected: {word}\")\n",
    "\n",
    "# Test\n",
    "test_word(\"aabb\")\n",
    "test_word(\"abb\")\n",
    "test_word(\"bab\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
