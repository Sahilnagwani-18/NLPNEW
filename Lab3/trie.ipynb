{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a85a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common prefix: statements\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}     \n",
    "        self.is_end = False\n",
    "        self.prefix_count = 0  \n",
    "\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "    def insert(self, word):\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "            node.prefix_count += 1\n",
    "        node.is_end = True\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self.root, f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            self.root = pickle.load(f)\n",
    "\n",
    "    def most_common_branch(self):\n",
    "        node = self.root\n",
    "        path = []\n",
    "        while node.children:\n",
    "\n",
    "            next_char, next_node = max(node.children.items(),\n",
    "                                       key=lambda item: item[1].prefix_count)\n",
    "            path.append(next_char)\n",
    "            node = next_node\n",
    "        return ''.join(path)\n",
    "\n",
    "    def _add_nodes(self, dot, node, node_id):\n",
    "        for char, child in node.children.items():\n",
    "            child_id = f'{node_id}_{char}'\n",
    "            label = f'{char}\\nCount:{child.prefix_count}'\n",
    "            if child.is_end:\n",
    "                label += '\\n(End)'\n",
    "            dot.node(child_id, label=label, shape='circle')\n",
    "            dot.edge(node_id, child_id)\n",
    "            self._add_nodes(dot, child, child_id)\n",
    "\n",
    "\n",
    "filename = 'brown_nouns.txt'  \n",
    "\n",
    "trie = Trie()\n",
    "with open(filename, 'r') as f:\n",
    "    for line in f:\n",
    "        word = line.strip()\n",
    "        if word:\n",
    "            trie.insert(word)\n",
    "\n",
    "trie.save('brown_trie.pkl')\n",
    "\n",
    "\n",
    "most_common_prefix = trie.most_common_branch()\n",
    "print(f'Most common prefix: {most_common_prefix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044445d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prefix splits found: 92116 saved in prefix.txt\n",
      "✅ Suffix splits found: 91740 saved in suffix.txt\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end = False\n",
    "        self.count = 0  \n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, word):\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node.children:\n",
    "                node.children[ch] = TrieNode()\n",
    "            node = node.children[ch]\n",
    "            node.count += 1\n",
    "        node.is_end = True\n",
    "\n",
    "    def find_prefix_split(self, word, words_set):\n",
    "        \"\"\"\n",
    "        Try to split word into stem+suffix using prefix trie\n",
    "        Only valid if stem exists in dataset and suffix branching is valid\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        for i in range(len(word)):\n",
    "            ch = word[i]\n",
    "            if ch not in node.children:\n",
    "                break\n",
    "            node = node.children[ch]\n",
    "\n",
    "            stem = word[:i+1]\n",
    "            suffix = word[i+1:]\n",
    "            if stem in words_set and node.count > 1 and suffix != \"\":\n",
    "                return f\"{stem}+{suffix}\"\n",
    "        return word\n",
    "\n",
    "\n",
    "class SuffixTrie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, word):\n",
    "        node = self.root\n",
    "        for ch in reversed(word):  \n",
    "            if ch not in node.children:\n",
    "                node.children[ch] = TrieNode()\n",
    "            node = node.children[ch]\n",
    "            node.count += 1\n",
    "        node.is_end = True\n",
    "\n",
    "    def find_suffix_split(self, word, words_set):\n",
    "        \"\"\"\n",
    "        Try to split word into stem+suffix using suffix trie\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        for i in range(len(word)):\n",
    "            ch = word[-(i+1)]\n",
    "            if ch not in node.children:\n",
    "                break\n",
    "            node = node.children[ch]\n",
    "\n",
    "            stem = word[:len(word)-(i+1)]\n",
    "            suffix = word[len(word)-(i+1):]\n",
    "\n",
    "            if stem in words_set and node.count > 1 and suffix != \"\":\n",
    "                return f\"{stem}+{suffix}\"\n",
    "        return word\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"brown_nouns.txt\") as f:\n",
    "        words = [w.strip().lower() for w in f if w.strip()]\n",
    "\n",
    "    words_set = set(words)\n",
    "\n",
    "    prefix_trie = Trie()\n",
    "    suffix_trie = SuffixTrie()\n",
    "\n",
    "    for w in words:\n",
    "        prefix_trie.insert(w)\n",
    "        suffix_trie.insert(w)\n",
    "\n",
    "    # Collect results\n",
    "    prefix_results = []\n",
    "    suffix_results = []\n",
    "\n",
    "    for w in words:\n",
    "        p_split = prefix_trie.find_prefix_split(w, words_set)\n",
    "        s_split = suffix_trie.find_suffix_split(w, words_set)\n",
    "\n",
    "        if \"+\" in p_split:  # valid prefix split\n",
    "            prefix_results.append(p_split)\n",
    "        if \"+\" in s_split:  # valid suffix split\n",
    "            suffix_results.append(s_split)\n",
    "\n",
    "    # Save to files\n",
    "    with open(\"prefix.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(prefix_results))\n",
    "\n",
    "    with open(\"suffix.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(suffix_results))\n",
    "\n",
    "    print(f\"✅ Prefix splits found: {len(prefix_results)} saved in prefix.txt\")\n",
    "    print(f\"✅ Suffix splits found: {len(suffix_results)} saved in suffix.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a4ec5bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: jury\n",
      "Prefix-based split: jury\n",
      "Suffix-based split: jury\n"
     ]
    }
   ],
   "source": [
    "# ---------------- CHECK SPECIFIC WORD ----------------\n",
    "query_word = \"jury\"\n",
    "\n",
    "p_split = prefix_trie.find_prefix_split(query_word, words_set)\n",
    "s_split = suffix_trie.find_suffix_split(query_word, words_set)\n",
    "\n",
    "print(\"Word:\", query_word)\n",
    "print(\"Prefix-based split:\", p_split)\n",
    "print(\"Suffix-based split:\", s_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8188def8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Prefix splits in prefix.txt, suffix splits in suffix.txt\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end = False\n",
    "        self.count = 0  # how many words pass through this node\n",
    "\n",
    "class Trie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, word):\n",
    "        node = self.root\n",
    "        for ch in word:\n",
    "            if ch not in node.children:\n",
    "                node.children[ch] = TrieNode()\n",
    "            node = node.children[ch]\n",
    "            node.count += 1\n",
    "        node.is_end = True\n",
    "\n",
    "    def top_k_splits(self, word, k=3):\n",
    "        \"\"\"\n",
    "        Return split form and stats for top-k splits.\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        split_candidates = []\n",
    "\n",
    "        for i, ch in enumerate(word):\n",
    "            if ch not in node.children:\n",
    "                break\n",
    "            node = node.children[ch]\n",
    "            split_candidates.append((i, node.count, ch))\n",
    "\n",
    "        # top k by count\n",
    "        top_splits = sorted(split_candidates, key=lambda x: -x[1])[:k]\n",
    "        split_indices = sorted([idx for idx, _, _ in top_splits])\n",
    "\n",
    "        # build parts\n",
    "        parts = []\n",
    "        last = 0\n",
    "        for idx in split_indices:\n",
    "            parts.append(word[last:idx+1])\n",
    "            last = idx+1\n",
    "        parts.append(word[last:])\n",
    "\n",
    "        # make split string\n",
    "        split_str = f\"{word}=\" + \"+\".join(parts)\n",
    "\n",
    "        # make frequency info\n",
    "        freq_info = \" | \".join([f\"{ch}@{cnt}\" for _, cnt, ch in top_splits])\n",
    "\n",
    "        return split_str, freq_info\n",
    "\n",
    "\n",
    "class SuffixTrie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "\n",
    "    def insert(self, word):\n",
    "        node = self.root\n",
    "        for ch in reversed(word):  # insert reversed\n",
    "            if ch not in node.children:\n",
    "                node.children[ch] = TrieNode()\n",
    "            node = node.children[ch]\n",
    "            node.count += 1\n",
    "        node.is_end = True\n",
    "\n",
    "    def top_k_splits(self, word, k=3):\n",
    "        \"\"\"\n",
    "        Return split form and stats for top-k splits (suffix trie).\n",
    "        \"\"\"\n",
    "        node = self.root\n",
    "        split_candidates = []\n",
    "        n = len(word)\n",
    "\n",
    "        for i, ch in enumerate(reversed(word)):\n",
    "            if ch not in node.children:\n",
    "                break\n",
    "            node = node.children[ch]\n",
    "            split_candidates.append((n - i - 1, node.count, ch))\n",
    "\n",
    "        # top k by count\n",
    "        top_splits = sorted(split_candidates, key=lambda x: -x[1])[:k]\n",
    "        split_indices = sorted([idx for idx, _, _ in top_splits])\n",
    "\n",
    "        # build parts\n",
    "        parts = []\n",
    "        last = 0\n",
    "        for idx in split_indices:\n",
    "            parts.append(word[last:idx+1])\n",
    "            last = idx+1\n",
    "        parts.append(word[last:])\n",
    "\n",
    "        # make split string\n",
    "        split_str = f\"{word}=\" + \"+\".join(parts)\n",
    "\n",
    "        # frequency info\n",
    "        freq_info = \" | \".join([f\"{ch}@{cnt}\" for _, cnt, ch in top_splits])\n",
    "\n",
    "        return split_str, freq_info\n",
    "\n",
    "\n",
    "# ---------------- MAIN ----------------\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"brown_nouns.txt\") as f:\n",
    "        words = [w.strip().lower() for w in f if w.strip()]\n",
    "\n",
    "    prefix_trie = Trie()\n",
    "    suffix_trie = SuffixTrie()\n",
    "\n",
    "    for w in words:\n",
    "        prefix_trie.insert(w)\n",
    "        suffix_trie.insert(w)\n",
    "\n",
    "    prefix_results = []\n",
    "    suffix_results = []\n",
    "\n",
    "    for w in words:\n",
    "        split_str, freq_info = prefix_trie.top_k_splits(w, k=3)\n",
    "        prefix_results.append(split_str + \"\\n\" + freq_info + \"\\n\")\n",
    "\n",
    "        split_str, freq_info = suffix_trie.top_k_splits(w, k=3)\n",
    "        suffix_results.append(split_str + \"\\n\" + freq_info + \"\\n\")\n",
    "\n",
    "    with open(\"prefix.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(prefix_results))\n",
    "\n",
    "    with open(\"suffix.txt\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(suffix_results))\n",
    "\n",
    "    print(\"✅ Done. Prefix splits in prefix.txt, suffix splits in suffix.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
