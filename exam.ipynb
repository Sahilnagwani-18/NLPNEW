{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf135fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example A: All counts missing (simulate unseen) ===\n",
      "Sentence tokens: ['Natural', 'Language', 'Processing', 'is', 'Love']\n",
      "Cutoff k = 1\n",
      "\n",
      "Predict P(Natural) -> unigram\n",
      "  Unigram count C(Natural) = 0\n",
      "  -> count <= k (1) : unigram too rare/unseen -> use UNIFORM fallback (stop)\n",
      "\n",
      "Predict P(Language | Natural) -> bigram\n",
      "  Bigram count C(Natural,Language) = 0; history count C(Natural) = 0\n",
      "  -> bigram count <= k or history unseen -> back off to unigram:\n",
      "  Unigram count C(Language) = 0\n",
      "  -> count <= k (1) : unigram too rare/unseen -> use UNIFORM fallback (stop)\n",
      "\n",
      "Predict P(Processing | Natural, Language) -> trigram\n",
      "  Trigram count C(Natural,Language,Processing) = 0; trigram-history count C(Natural,Language) = 0\n",
      "  -> trigram count <= k or trigram-history unseen -> back off to bigram:\n",
      "  Bigram count C(Language,Processing) = 0; history count C(Language) = 0\n",
      "  -> bigram count <= k or history unseen -> back off to unigram:\n",
      "  Unigram count C(Processing) = 0\n",
      "  -> count <= k (1) : unigram too rare/unseen -> use UNIFORM fallback (stop)\n",
      "\n",
      "Predict P(is | Language, Processing) -> trigram\n",
      "  Trigram count C(Language,Processing,is) = 0; trigram-history count C(Language,Processing) = 0\n",
      "  -> trigram count <= k or trigram-history unseen -> back off to bigram:\n",
      "  Bigram count C(Processing,is) = 0; history count C(Processing) = 0\n",
      "  -> bigram count <= k or history unseen -> back off to unigram:\n",
      "  Unigram count C(is) = 0\n",
      "  -> count <= k (1) : unigram too rare/unseen -> use UNIFORM fallback (stop)\n",
      "\n",
      "Predict P(Love | Processing, is) -> trigram\n",
      "  Trigram count C(Processing,is,Love) = 0; trigram-history count C(Processing,is) = 0\n",
      "  -> trigram count <= k or trigram-history unseen -> back off to bigram:\n",
      "  Bigram count C(is,Love) = 0; history count C(is) = 0\n",
      "  -> bigram count <= k or history unseen -> back off to unigram:\n",
      "  Unigram count C(Love) = 0\n",
      "  -> count <= k (1) : unigram too rare/unseen -> use UNIFORM fallback (stop)\n",
      "\n",
      "\n",
      "=== Example B: Some counts provided ===\n",
      "Sentence tokens: ['Natural', 'Language', 'Processing', 'is', 'Love']\n",
      "Cutoff k = 1\n",
      "\n",
      "Predict P(Natural) -> unigram\n",
      "  Unigram count C(Natural) = 1\n",
      "  -> count <= k (1) : unigram too rare/unseen -> use UNIFORM fallback (stop)\n",
      "\n",
      "Predict P(Language | Natural) -> bigram\n",
      "  Bigram count C(Natural,Language) = 1; history count C(Natural) = 1\n",
      "  -> bigram count <= k or history unseen -> back off to unigram:\n",
      "  Unigram count C(Language) = 5\n",
      "  -> count > k (1) : use discounted unigram MLE (stop)\n",
      "\n",
      "Predict P(Processing | Natural, Language) -> trigram\n",
      "  Trigram count C(Natural,Language,Processing) = 0; trigram-history count C(Natural,Language) = 1\n",
      "  -> trigram count <= k or trigram-history unseen -> back off to bigram:\n",
      "  Bigram count C(Language,Processing) = 3; history count C(Language) = 5\n",
      "  -> bigram count > k : use discounted bigram MLE (stop)\n",
      "\n",
      "Predict P(is | Language, Processing) -> trigram\n",
      "  Trigram count C(Language,Processing,is) = 2; trigram-history count C(Language,Processing) = 3\n",
      "  -> trigram count > k : use discounted trigram MLE (stop)\n",
      "\n",
      "Predict P(Love | Processing, is) -> trigram\n",
      "  Trigram count C(Processing,is,Love) = 0; trigram-history count C(Processing,is) = 2\n",
      "  -> trigram count <= k or trigram-history unseen -> back off to bigram:\n",
      "  Bigram count C(is,Love) = 0; history count C(is) = 3\n",
      "  -> bigram count <= k or history unseen -> back off to unigram:\n",
      "  Unigram count C(Love) = 0\n",
      "  -> count <= k (1) : unigram too rare/unseen -> use UNIFORM fallback (stop)\n"
     ]
    }
   ],
   "source": [
    "# Katz-path-simulator.py\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "class KatzPathSimulator:\n",
    "    def __init__(self, k:int=1):\n",
    "        \"\"\"\n",
    "        k : cutoff for Katz. If count > k we use discounted MLE,\n",
    "            otherwise we back off to lower order.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def _get_count(self, counts:Dict, key):\n",
    "        # safe getter: missing = 0\n",
    "        return counts.get(key, 0)\n",
    "\n",
    "    def explain_sentence(self, sentence: str,\n",
    "                         trigram_counts: Dict[Tuple[str,str,str], int] = None,\n",
    "                         bigram_counts: Dict[Tuple[str,str], int] = None,\n",
    "                         unigram_counts: Dict[str,int] = None):\n",
    "        \"\"\"\n",
    "        Prints a step-by-step call trace of Katz backoff decisions.\n",
    "        If any counts dict is None it will be treated as empty (all zeros).\n",
    "        \"\"\"\n",
    "        if trigram_counts is None: trigram_counts = {}\n",
    "        if bigram_counts is None: bigram_counts = {}\n",
    "        if unigram_counts is None: unigram_counts = {}\n",
    "\n",
    "        tokens = sentence.split()\n",
    "        n = len(tokens)\n",
    "        if n == 0:\n",
    "            print(\"Empty sentence.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Sentence tokens: {tokens}\\nCutoff k = {self.k}\\n\")\n",
    "\n",
    "        # For first token, only unigram\n",
    "        print(f\"Predict P({tokens[0]}) -> unigram\")\n",
    "        self._explain_unigram(tokens[0], unigram_counts)\n",
    "\n",
    "        if n >= 2:\n",
    "            print(\"\\nPredict P({} | {}) -> bigram\".format(tokens[1], tokens[0]))\n",
    "            self._explain_bigram(tokens[1], tokens[0], bigram_counts, unigram_counts)\n",
    "\n",
    "        # from token index 2 onwards, use trigram history (h1,h2)\n",
    "        for i in range(2, n):\n",
    "            w = tokens[i]\n",
    "            h1, h2 = tokens[i-2], tokens[i-1]\n",
    "            print(f\"\\nPredict P({w} | {h1}, {h2}) -> trigram\")\n",
    "            self._explain_trigram(w, h1, h2, trigram_counts, bigram_counts, unigram_counts)\n",
    "\n",
    "    def _explain_unigram(self, w, unigram_counts):\n",
    "        c = self._get_count(unigram_counts, w)\n",
    "        print(f\"  Unigram count C({w}) = {c}\")\n",
    "        if c > self.k:\n",
    "            print(f\"  -> count > k ({self.k}) : use discounted unigram MLE (stop)\")\n",
    "        else:\n",
    "            print(f\"  -> count <= k ({self.k}) : unigram too rare/unseen -> use UNIFORM fallback (stop)\")\n",
    "\n",
    "    def _explain_bigram(self, w, h, bigram_counts, unigram_counts):\n",
    "        c = self._get_count(bigram_counts, (h,w))\n",
    "        c_h = self._get_count(unigram_counts, h)\n",
    "        print(f\"  Bigram count C({h},{w}) = {c}; history count C({h}) = {c_h}\")\n",
    "        if c > self.k and c_h > 0:\n",
    "            print(f\"  -> bigram count > k : use discounted bigram MLE (stop)\")\n",
    "        else:\n",
    "            print(f\"  -> bigram count <= k or history unseen -> back off to unigram:\")\n",
    "            self._explain_unigram(w, unigram_counts)\n",
    "\n",
    "    def _explain_trigram(self, w, h1, h2, trigram_counts, bigram_counts, unigram_counts):\n",
    "        c = self._get_count(trigram_counts, (h1,h2,w))\n",
    "        c_h = self._get_count(bigram_counts, (h1,h2))\n",
    "        print(f\"  Trigram count C({h1},{h2},{w}) = {c}; trigram-history count C({h1},{h2}) = {c_h}\")\n",
    "        if c > self.k and c_h > 0:\n",
    "            print(f\"  -> trigram count > k : use discounted trigram MLE (stop)\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"  -> trigram count <= k or trigram-history unseen -> back off to bigram:\")\n",
    "            self._explain_bigram(w, h2, bigram_counts, unigram_counts)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Example runs (you can edit counts to test different paths)\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    sim = KatzPathSimulator(k=1)\n",
    "\n",
    "    sentence = \"Natural Language Processing is Love\"\n",
    "\n",
    "    # Example A: all counts unknown (empty dicts) -> everything unseen\n",
    "    print(\"=== Example A: All counts missing (simulate unseen) ===\")\n",
    "    sim.explain_sentence(sentence)\n",
    "\n",
    "    # Example B: partial counts so some trigram/bigram exist\n",
    "    print(\"\\n\\n=== Example B: Some counts provided ===\")\n",
    "    trigram_counts = {\n",
    "        # Suppose only (\"Language\",\"Processing\",\"is\") seen 2 times\n",
    "        (\"Language\",\"Processing\",\"is\"): 2,\n",
    "        # (\"Natural\",\"Language\",\"Processing\") absent (0)\n",
    "    }\n",
    "    bigram_counts = {\n",
    "        (\"Natural\",\"Language\"): 1,     # small\n",
    "        (\"Language\",\"Processing\"): 3,  # bigger\n",
    "        (\"Processing\",\"is\"): 2,\n",
    "        (\"is\",\"Love\"): 0\n",
    "    }\n",
    "    unigram_counts = {\n",
    "        \"Natural\": 1,\n",
    "        \"Language\": 5,\n",
    "        \"Processing\": 2,\n",
    "        \"is\": 3,\n",
    "        \"Love\": 0   # unseen unigram\n",
    "    }\n",
    "    sim.explain_sentence(sentence,\n",
    "                         trigram_counts=trigram_counts,\n",
    "                         bigram_counts=bigram_counts,\n",
    "                         unigram_counts=unigram_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cc3ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Vocabulary:\n",
      "l o w </w> : 1\n",
      "l o w e r </w> : 1\n",
      "n e w e s t </w> : 1\n",
      "w i d e s t </w> : 1\n",
      "\n",
      "Merge 1: ('l', 'o')\n",
      "lo w </w> : 1\n",
      "lo w e r </w> : 1\n",
      "n e w e s t </w> : 1\n",
      "w i d e s t </w> : 1\n",
      "\n",
      "Merge 2: ('lo', 'w')\n",
      "low </w> : 1\n",
      "low e r </w> : 1\n",
      "n e w e s t </w> : 1\n",
      "w i d e s t </w> : 1\n",
      "\n",
      "Merge 3: ('e', 's')\n",
      "low </w> : 1\n",
      "low e r </w> : 1\n",
      "n e w es t </w> : 1\n",
      "w i d es t </w> : 1\n",
      "\n",
      "Merge 4: ('es', 't')\n",
      "low </w> : 1\n",
      "low e r </w> : 1\n",
      "n e w est </w> : 1\n",
      "w i d est </w> : 1\n",
      "\n",
      "Merge 5: ('est', '</w>')\n",
      "low </w> : 1\n",
      "low e r </w> : 1\n",
      "n e w est</w> : 1\n",
      "w i d est</w> : 1\n",
      "\n",
      "Merge 6: ('low', '</w>')\n",
      "low</w> : 1\n",
      "low e r </w> : 1\n",
      "n e w est</w> : 1\n",
      "w i d est</w> : 1\n",
      "\n",
      "Merge 7: ('low', 'e')\n",
      "low</w> : 1\n",
      "lowe r </w> : 1\n",
      "n e w est</w> : 1\n",
      "w i d est</w> : 1\n",
      "\n",
      "Merge 8: ('lowe', 'r')\n",
      "low</w> : 1\n",
      "lower </w> : 1\n",
      "n e w est</w> : 1\n",
      "w i d est</w> : 1\n",
      "\n",
      "Merge 9: ('lower', '</w>')\n",
      "low</w> : 1\n",
      "lower</w> : 1\n",
      "n e w est</w> : 1\n",
      "w i d est</w> : 1\n",
      "\n",
      "Merge 10: ('n', 'e')\n",
      "low</w> : 1\n",
      "lower</w> : 1\n",
      "ne w est</w> : 1\n",
      "w i d est</w> : 1\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"Count frequency of symbol pairs in the vocabulary.\"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, vocab):\n",
    "    \"\"\"Merge the most frequent pair in vocabulary.\"\"\"\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    new_vocab = {}\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "\n",
    "# Step 1: Build initial vocabulary with character-level tokens\n",
    "vocab = {}\n",
    "for word in corpus:\n",
    "    chars = \" \".join(list(word)) + \" </w>\"   # </w> marks word-end\n",
    "    vocab[chars] = vocab.get(chars, 0) + 1\n",
    "\n",
    "print(\"Initial Vocabulary:\")\n",
    "for k, v in vocab.items():\n",
    "    print(k, \":\", v)\n",
    "\n",
    "# Step 2: Run BPE merges\n",
    "num_merges = 10\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)   # most frequent pair\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print(f\"\\nMerge {i+1}: {best}\")\n",
    "    for k, v in vocab.items():\n",
    "        print(k, \":\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5617b7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned merges: [('l', 'o'), ('lo', 'w'), ('e', 's'), ('es', 't'), ('est', '</w>'), ('low', '</w>'), ('low', 'e'), ('lowe', 'r'), ('lower', '</w>'), ('n', 'e')]\n",
      "\n",
      "Final Subword Vocabulary:\n",
      "{'##low</w>', 'l', 'est', '##low', 'est</w>', '##ne', 'n', '##es', '##est</w>', 't', '##lo', 'i', 'es', 'low</w>', 'r', '##lower', 'lower</w>', 'low', 'w', 'lo', 'lower', '##lowe', 's', 'ne', '##est', 'o', '##lower</w>', 'e', 'd', 'lowe'}\n",
      "\n",
      "Tokenization examples:\n",
      "lower -> ['lower']\n",
      "newest -> ['ne', '[UNK]']\n",
      "widest -> ['w', '[UNK]']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# -------------------------\n",
    "# Step 1: Build initial vocab from corpus\n",
    "# -------------------------\n",
    "def get_vocab(corpus):\n",
    "    vocab = {}\n",
    "    for word in corpus:\n",
    "        chars = \" \".join(list(word)) + \" </w>\"\n",
    "        vocab[chars] = vocab.get(chars, 0) + 1\n",
    "    return vocab\n",
    "\n",
    "# -------------------------\n",
    "# Step 2: Compute pair scores (WordPiece style)\n",
    "# -------------------------\n",
    "def compute_scores(vocab):\n",
    "    pair_counts = defaultdict(int)\n",
    "    total_freq = sum(vocab.values())\n",
    "\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pair = (symbols[i], symbols[i+1])\n",
    "            pair_counts[pair] += freq\n",
    "\n",
    "    scores = {}\n",
    "    for pair, count in pair_counts.items():\n",
    "        # simplified score = probability of this pair in corpus\n",
    "        scores[pair] = count / total_freq\n",
    "    return scores\n",
    "\n",
    "# -------------------------\n",
    "# Step 3: Merge vocab with best pair\n",
    "# -------------------------\n",
    "def merge_vocab(pair, vocab):\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    new_vocab = {}\n",
    "    for word in vocab:\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_vocab[new_word] = vocab[word]\n",
    "    return new_vocab\n",
    "\n",
    "# -------------------------\n",
    "# Step 4: Train WordPiece merges\n",
    "# -------------------------\n",
    "def train_wordpiece(corpus, num_merges=20):\n",
    "    vocab = get_vocab(corpus)\n",
    "    merges = []\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        scores = compute_scores(vocab)\n",
    "        if not scores:\n",
    "            break\n",
    "        best = max(scores, key=scores.get)\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "        merges.append(best)\n",
    "    return merges\n",
    "\n",
    "# -------------------------\n",
    "# Step 5: Build final subword vocab\n",
    "# -------------------------\n",
    "def build_subword_vocab(merges, corpus):\n",
    "    # Start with characters\n",
    "    subword_vocab = set(list(\"\".join(corpus)))\n",
    "    for (a, b) in merges:\n",
    "        token = a + b\n",
    "        subword_vocab.add(token)\n",
    "    return subword_vocab\n",
    "\n",
    "# -------------------------\n",
    "# Step 6: WordPiece Tokenizer\n",
    "# -------------------------\n",
    "def tokenize_wordpiece(word, vocab):\n",
    "    tokens = []\n",
    "    start = 0\n",
    "    while start < len(word):\n",
    "        end = len(word)\n",
    "        subword = None\n",
    "        while start < end:\n",
    "            piece = word[start:end]\n",
    "            if start > 0:  # continuation â†’ add ##\n",
    "                piece = \"##\" + piece\n",
    "            if piece in vocab:\n",
    "                subword = piece\n",
    "                break\n",
    "            end -= 1\n",
    "        if subword is None:\n",
    "            tokens.append(\"[UNK]\")\n",
    "            break\n",
    "        tokens.append(subword)\n",
    "        start = end if subword.startswith(\"##\") else len(subword)\n",
    "    return tokens\n",
    "\n",
    "# -------------------------\n",
    "# Example Run\n",
    "# -------------------------\n",
    "corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "\n",
    "# Train\n",
    "merges = train_wordpiece(corpus, num_merges=10)\n",
    "print(\"Learned merges:\", merges)\n",
    "\n",
    "# Build vocab\n",
    "subword_vocab = build_subword_vocab(merges, corpus)\n",
    "# Add common prefixes with ##\n",
    "extended_vocab = set(subword_vocab)\n",
    "for token in list(subword_vocab):\n",
    "    if len(token) > 1:\n",
    "        extended_vocab.add(\"##\" + token)\n",
    "\n",
    "print(\"\\nFinal Subword Vocabulary:\")\n",
    "print(extended_vocab)\n",
    "\n",
    "# Tokenize words\n",
    "print(\"\\nTokenization examples:\")\n",
    "print(\"lower ->\", tokenize_wordpiece(\"lower\", extended_vocab))\n",
    "print(\"newest ->\", tokenize_wordpiece(\"newest\", extended_vocab))\n",
    "print(\"widest ->\", tokenize_wordpiece(\"widest\", extended_vocab))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
